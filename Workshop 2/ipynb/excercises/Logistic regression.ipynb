{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Previously, we've looked at linear regression, useful for predicting *continuous variables*. In this workshop we look at logistic regression, with which we can predict *categorical variables* - in other words: *classification*. We'll see that all the concepts introduced for linear regression (e.g. hypothesis, cost function, learning, ...) are useful for logistic regression as well. \n",
    "\n",
    "We'll see that logistic regression is actually a single neuron of a neural network. So it's a single decision unit, capable of making simple decisions, whereas a neural network, consisting of multiple neurons, is capable of making complex decisions (i.e. decisions that are based on information of earlier decisions).\n",
    "\n",
    "This workshop is based on Andrew Ng’s Machine Learning Coursera course. While watching the videos, make notes of the parts that you don’t understand. We’ll discuss these in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please watch the following videos (~60 min.): \n",
    "- [Logistic regression: classification](https://www.youtube.com/watch?v=-la3q9d7AKQ&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=32) (8 min.). Concepts: classification.\n",
    "- [Logistic regression: hypothesis representation](https://www.youtube.com/watch?v=t1IT5hZfS48&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=33) (7 min.). Logistic regression uses the sigmoid function as hypothesis. The hypothesis represents the estimated probability that y=1 on input x.\n",
    "- [Logistic regression: decision boundary](https://www.youtube.com/watch?v=F_VG4LNjZZw&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=34) (11 min.). Concepts: decision boundary. Please note that if z is a linear function, the decision boundary is a straight line for two features or a flat plane for three features. Non-linear decision boundaries are possible by adding polynomial terms to the z.\n",
    "- [Logistic regression: cost function](https://www.youtube.com/watch?v=HIQlmHxI6-0&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=35) (11 min.). If we use the cost function that we've used for linear regression, the cost function is not guaranteed to be convex: learning might end up in a local minimum. Therefore the logistic regression cost function is used, which is convex.\n",
    "- [Logistic regression: simplified cost function and gradient descent](https://www.youtube.com/watch?v=TTdcc21Ko9A&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=36) (10 min.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case description\n",
    "Suppose that you are the administrator of a university department and you want to determine each applicant’s chance of admission based on their results on two exams. You have historical data from previous applicants that you can use as a training set for logistic regression. For each training example, you have the applicant’s scores on two exams and the admissions decision.\n",
    "\n",
    "Your task is to build a classification model that estimates an applicant’s probability of admission based on the scores from those two exams.\n",
    "\n",
    "**Exercise**:\n",
    "* read the training data ```ex2data1.txt```. It has two features and one output. The first feature is the result of the 1st exam (float between 0-100). The second feature is the result of the 2nd exam (float between 0-100). The output is the admission to the university (integer; 0 means no admission, 1 means admission granted). \n",
    "* Save the data in ```X``` and ```y```.\n",
    "* ```df.head()``` is a convenient way to inspect the first couple of training samples.\n",
    "\n",
    "**Exercise**: visualize the data. Use a scatter plot with \"exam 1 score\" on the x-axis, \"exam 2 score\" on the y-axis, and two colours for \"admission\" and \"no admission\". An easy way to separate admitted and not-admitted values in python is by using a mask:\n",
    "  ```\n",
    "  mask = y == 1  # y is a column vector, \n",
    "  adm = plt.scatter(X[mask][0].values, X[mask][1].values)\n",
    "  ```\n",
    "The plot should look similar to:\n",
    "<img src=\"dataVisualization.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# read the training data\n",
    "# your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the data\n",
    "# your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the data\n",
    "# your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the training set\n",
    "(m, n) = X.shape\n",
    "ones = np.ones((m, 1))\n",
    "X = np.hstack((ones, X))  # add X_0\n",
    "(m, n) = X.shape\n",
    "y = y[:, np.newaxis]  # convert from vector to matrix; not needed for X, as X is already matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're lazy and skip further data exploration and wrangling and assume that there's no missing data and that there are no outliers. \n",
    "\n",
    "We also have already decided that both features are selected for the prediction.\n",
    "\n",
    "Both features have the same range, so we skip feature normalization (lazy again)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: as explained in the video's, logistic regression uses the sigmoid function. Implement the body of the sigmoid function. The picture shows a number of often-used activation functions.\n",
    "![activationFunctions](./activationFunctions.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by convention, call the function parameter z\n",
    "def sigmoid(z):\n",
    "    # your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: for linear regression, the hypothesis function was the dot product of $X$ and $\\Theta$. For logistic regression, the hypothesis function is the dot product of $X$ and $\\Theta$ and the result is passed to the sigmoid function. Implement the body of the hypothesis function for logistic regression, using the sigmoid function defined just above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypothesis\n",
    "def h(X, theta):\n",
    "    # your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: implement the body of cost function for logistic regression. You can do this based on the picture just below, which already looks quite impressive. The video's have explained why the cost function for logistic regression has this form. To implement the body of the function, you don't need to understand this. You \"only\" need to transform the mathematical notation into python.\n",
    "* Use ```np.log``` to calculate the logarithm.\n",
    "* Use ```np.multiply``` to element-wise multipy ```y``` and ```np.log(h(X, theta))```.\n",
    "* Use ```np.multiply``` to element-wise multipy ```1-y``` and ```np.log(1 - h(X, theta))```.\n",
    "* Use ```np.sum``` to do the summation.\n",
    "* Don't forget to multiply by ```(-1/m)```\n",
    "\n",
    "![logisticRegressionCostFunction](./logisticRegressionCostFunction.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost function\n",
    "def J(X, y, theta):\n",
    "    # your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: implement the body of the gradient function for logistic regression, based on the picture just below. As you have seen in the video's, the gradient is identical to the one of linear regression (except that $h$ is the sigmoid function instead of a linear function)! So this is an easy exercise.\n",
    "\n",
    "![gradient](./gradient.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient\n",
    "def grad(X, y, theta):\n",
    "    # your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freehand test of cost function and gradient\n",
    "theta = np.zeros((n, 1)) # intializing theta with all zeros\n",
    "print(J(X, y, theta))  # should give 0.6931471805599453\n",
    "np.testing.assert_array_almost_equal_nulp(J(X, y, theta), 0.6931471805599453)\n",
    "print(grad(X, y, theta))  # should give [[-0.1] [-12.009216589291151] [-11.262842205513593]]\n",
    "np.testing.assert_array_almost_equal_nulp(grad(X, y, theta), [[-0.1], [-12.009216589291151], [-11.262842205513593]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning algorithm for logsitic regression (identical to the one in the hands-on about linear regression!)\n",
    "def gradientDescent(X, y, theta, alpha, num_iters):\n",
    "    J_history = []\n",
    "    for _ in range(num_iters):\n",
    "        theta -= alpha * grad(X, y, theta)\n",
    "        J_history.append(J(X, y, theta))  # to allow displaying cost as a function of #iters\n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# perform learning (identical to the one in the hands-on about linear regression!)\n",
    "num_iters = 150000 # hyper parameter number of iterations\n",
    "alpha = 0.002  # hyper parameter learning rate\n",
    "theta, J_history = gradientDescent(X, y, theta, alpha, num_iters)\n",
    "print(\"theta: \" + str(theta))  # should be [[-10.901] [0.0925] [0.0864]]\n",
    "print(\"cost J: \" + str(J(X, y, theta)))  # should be 0.261"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cost function as a function of #iterations\n",
    "plt.plot(J_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"$J(\\Theta)$\")\n",
    "plt.title(\"Cost function convergence as a function of #iterations\")\n",
    "plt.show()  # please notice the problems the algorithm has to converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot decision boundary\n",
    "mask = y.flatten() == 1\n",
    "adm = plt.scatter(X[mask][:,1], X[mask][:,2])\n",
    "not_adm = plt.scatter(X[~mask][:,1], X[~mask][:,2])\n",
    "\n",
    "x_value = np.array([np.min(X[:,1]), np.max(X[:,1])])\n",
    "y_value = -(theta[0] + theta[1] * x_value)/theta[2]\n",
    "plt.plot(x_value, y_value, \"r\")\n",
    "\n",
    "plt.xlabel('Exam 1 score')\n",
    "plt.ylabel('Exam 2 score')\n",
    "plt.legend((adm, not_adm), ('Admitted', 'Not admitted'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X, y, theta):\n",
    "    pred = h(X, theta) >= 0.5\n",
    "    print(\"type(pred):\", pred[0], y[0])\n",
    "    acc = np.mean(np.array(pred) == y)\n",
    "    return acc * 100\n",
    "\n",
    "print(\"accuracy = \" + str(accuracy(X, y, theta)))  # should be 91%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just above, the plot that shows cost function as a function of #iterations, had some problems to converge. Let's try a more sophisticated optimizer than gradient descent: fmin_tnc from the scipy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform learning, but use a more sophisticated optimizer than gradient descent: fmin_tnc\n",
    "import scipy.optimize as opt\n",
    "\n",
    "# Slight inconvenience, fmin_tnc expects the parameters of the cost function and the gradient in a different order. \n",
    "# We redfine the functions with the required parameter order\n",
    "def J_(theta, X, y):\n",
    "    return J(X, y, theta)\n",
    "def grad_(theta, X, y):\n",
    "    return grad(X, y, theta)\n",
    "\n",
    "# to allow displaying cost as a function of #iters\n",
    "J_history = []\n",
    "def callbackF(theta):\n",
    "    J_history.append(J(X, y, theta[:, np.newaxis]))  # transform theta from vector to matrix\n",
    "    \n",
    "theta = np.zeros((n, 1))  # re-intialize theta \n",
    "result = opt.fmin_tnc(func=J_, \n",
    "                    x0=theta.flatten(), \n",
    "                    fprime=grad_, \n",
    "                    args=(X, y.flatten()),\n",
    "                    callback=callbackF)\n",
    "\n",
    "theta = result[0][:, np.newaxis]  # transform theta from vector to matrix\n",
    "print(\"number of performed evaluation steps \" + str(result[1])) \n",
    "print(\"theta optimized: \" + str(theta))  # should be [-25.16131862   0.20623159   0.20147149]\n",
    "print(\"cost J: \" + str(J(X, y, theta)))  # should be 0.20349770158947464"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "* plot cost function as a function of #iterations\n",
    "* plot decision boundary\n",
    "* calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cost function as a function of #iterations\n",
    "# your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot decision boundary\n",
    "# your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "print(\"accuracy = \" + str(accuracy(X, y, theta)))  # should be 89%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Discussion**: when defining the hypothesis we've chosen:\n",
    "* $z = x \\cdot \\Theta$ (so a linear function of the features x1 and x2)\n",
    "* $h_\\Theta(x) = sigmoid(z)$\n",
    "This means that the decision boundary is a straight line. Considering the obtained accuracy, this seems to be a fair choice.\n",
    "\n",
    "We could add polynomial terms $(x_1)^2$, $x_1x_2$ and $(x_2)^2$ as additional features. This allows the decision boundary to become curved, which allows for a higher accuracy on the training set. We could even add higher order polynomial terms like x1^3 and x2^3 as additional features. Continuing to do so, we can reach an accuracy of 100%. \n",
    "\n",
    "**Question**: do you think this is a wise move?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please don’t look at it yet, but the Jupyter notebook with the solution is also available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
