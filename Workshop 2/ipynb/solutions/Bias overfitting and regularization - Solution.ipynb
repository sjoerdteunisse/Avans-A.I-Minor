{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Bias overfitting and regularization - Solution.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0UAg8zmnznH",
        "colab_type": "text"
      },
      "source": [
        "# Bias, overfitting and regularization\n",
        "\n",
        "Our machine learning algorithm learns from the training set and can then make predictions for new inputs. However, there's a risk that the algorithm has learned about the training set so well, that it will not generalize to new inputs, and consequently performs poorly. This is a major concern in machine learning that we'll address next. \n",
        "\n",
        "Please watch the following videos (~20 min.):\n",
        "- [Regularization: the problem of overfitting](https://www.youtube.com/watch?v=u73PU6Qwl1I&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=39) (9 min.). Concepts to grasp:\n",
        "    * Overfitting (high variance), underfitting (high bias). \n",
        "    * Too many features (different features from the domain or constructed features like polynomials) -> risk of overfitting!! Prevention: reduce number of features by only selecting the features that really matter. This can be done manually, or automatically. Automatic methods: correlation (heat map), dimensionality reduction (PCA), .... More info [here](https://towardsdatascience.com/feature-selection-and-dimensionality-reduction-f488d1a035de). Of course, by ignoring features, important information might be lost!\n",
        "    * Too few training samples -> risk for overfitting!! Counter measures: artificially increase the number of training samples.\n",
        "    * With many features, plotting the data is not possible any more to see if overfitting occurs.\n",
        "    * Careful model selection to prevent overfitting.\n",
        "- [Regularization: cost function](https://www.youtube.com/watch?v=KvtGD37Rm5I&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=40) (10 min.). Concepts to grasp: regularization. Regularization parameter lambda is a trade-off between fitting the training set wel and prevent overfitting. On other words lambda is a trade-off between bias and high variance. By convention, theta_0 is not regularized. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrUw6NoKnznH",
        "colab_type": "text"
      },
      "source": [
        "# Case description\n",
        "You will implement regularized logistic regression to predict whether microchips from a fabrication plant pass quality assurance (QA). During QA, each microchip goes through various tests to ensure it is functioning correctly.\n",
        "\n",
        "Suppose you are the product manager of the factory and you have the test results for some microchips on two different tests. From these two tests, you would like to determine whether the microchips should be accepted or rejected. To help you make the decision, you have a dataset of test results on microchips fabricated in the past, from which you can build a logistic regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ryf0v7NnznI",
        "colab_type": "text"
      },
      "source": [
        "**Exercise**:\n",
        "* read the training data. Feature 1 (column 0) is the result of test 1. Feature 2 (column 1) is the result of test 2. The third column is the label and determines whether the microchip was accepted or rejected.\n",
        "* save the data in ```X``` and ```y```.\n",
        "* explore the training data using ```df.head()```.\n",
        "* visualize the training data. x-axis: Microchip Test1, y-axis: Microchip Test2', use a mask to give passed and rejected microchips a different colour.\n",
        "\n",
        "The plot should look similar to:\n",
        "<img src=\"https://github.com/sjoerdteunisse/Avans-A.I-Minor/blob/master/Workshop%202/images/dataVisualization2.png?raw=true\" />\n",
        "\n",
        "The figure shows that our dataset cannot be separated into positive and negative examples by a straight line through the plot. Therefore, a straightforward application of logistic regression will not perform well on this dataset, as logistic regression will only be able to find a linear decision boundary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKgcXIoXnznI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# read the training data\n",
        "df = pd.read_csv('ex2data2.txt', header = None)  # \"df\" is short for data frame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NDtJjlbnznM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prepare the training set\n",
        "X = df.iloc[:,:-1]\n",
        "y = df.iloc[:,2]\n",
        "y = y[:, np.newaxis]\n",
        "\n",
        "# explore the data\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Xg1dtEBnznO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualize the data\n",
        "mask = y == 1\n",
        "passed = plt.scatter(X[mask][0].values, X[mask][1].values)\n",
        "failed = plt.scatter(X[~mask][0].values, X[~mask][1].values)\n",
        "plt.xlabel('Microchip Test1')\n",
        "plt.ylabel('Microchip Test2')\n",
        "plt.legend((passed, failed), ('Passed', 'Failed'))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zQ6OS99nznQ",
        "colab_type": "text"
      },
      "source": [
        "## Feature mapping\n",
        "One way to fit the data better is to create more features from each data point. Hence we will map the features into all polynomial terms of $x_1$ and $x_2$ up to the sixth power.\n",
        "\n",
        "<img src=\"https://github.com/sjoerdteunisse/Avans-A.I-Minor/blob/master/Workshop%202/images/featureMapping.png?raw=true\"/>\n",
        "\n",
        "As a result of this mapping, our vector of two features (the scores on two QA tests) has been transformed into a 28-dimensional vector. A logistic regression classifier trained on this higher-dimension feature vector will have a more complex decision boundary and will appear nonlinear when drawn in our 2-dimensional plot.\n",
        "\n",
        "While the feature mapping allows us to build a more expressive classifier, it is also more susceptible to overfitting. In the next parts of the exercise, you will implement regularized logistic regression to fit the data and also see for yourself how regularization can help combat the overfitting problem.\n",
        "\n",
        "We'll do feature mapping for polynomials up to degree 6. This can easily be done by using [sklearn.preprocessing.PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkuaFEK7nznQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# feature mapping\n",
        "degree = 6\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "poly_reg = PolynomialFeatures(degree)\n",
        "X = poly_reg.fit_transform(X)\n",
        "(m, n) = X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbXUE9-lnznS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sigmoid and hypothesis functions are copied from the previous hands-on, as they are not different for regularized logistic regression\n",
        "\n",
        "# by convention, call the function parameter z\n",
        "def sigmoid(z):\n",
        "    return 1/(1 + np.exp(-z))\n",
        "\n",
        "# hypothesis\n",
        "def h(X, theta):\n",
        "    return sigmoid(X @ theta)  # alternative way of writing dot product"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiLwTxjvnznU",
        "colab_type": "text"
      },
      "source": [
        "The cost function for logistic regression (previous hands-on) has been extended by a regularization term to avoid overfitting. By convention $\\Theta_0$ is not used in the regularization term:\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/sjoerdteunisse/Avans-A.I-Minor/blob/master/Workshop%202/images/logisticRegressionCostFunctionRegularized.png?raw=true\"/>\n",
        "\n",
        "\n",
        "This is called L2 regularization, because the regularization term squares the thetas.\n",
        "\n",
        "**Exercise**: implement the body of the regularized cost function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL2xmHnGnznU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# regularized cost function\n",
        "def Jreg(X, y, theta, lambdaa):\n",
        "    J = (-1/m) * (y.T @ np.log(h(X, theta)) + (1 - y.T) @ np.log(1 - h(X, theta)))\n",
        "    reg = (lambdaa/(2*m)) * (theta[1:].T @ theta[1:])  # by convention, skip theta_0\n",
        "    J = J + reg\n",
        "    return J\n",
        "\n",
        "# previously we've written the following for the cost J; similar outcome, different python notation:\n",
        "# J = (-1/m) * np.sum(np.multiply(y, np.log(h(X, theta))) + np.multiply((1-y), np.log(1 - h(X, theta))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HqY5WHnnznW",
        "colab_type": "text"
      },
      "source": [
        "The gradient for logistic regression (previous hands-on) has been extended by a regularization term to avoid overfitting. By convention $\\Theta_0$ is not used in the regularization term:\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/sjoerdteunisse/Avans-A.I-Minor/blob/master/Workshop%202/images/gradientRegularized.png?raw=true\"/>\n",
        "\n",
        "\n",
        "**Exercise**: implement the body of the regularized gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDji347inznW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gradient\n",
        "def gradReg(X, y, theta, lambdaa):\n",
        "    grad = np.zeros([m,1])\n",
        "    grad = (1/m) * X.T @ (h(X, theta) - y)  # simultaneous update\n",
        "    grad[1:] = grad[1:] + (lambdaa / m) * theta[1:]\n",
        "    return grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XsYQghjnznY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# freehand test of cost function and gradient\n",
        "theta = np.zeros((n, 1))\n",
        "lambdaa = 0  # lambda is a python keyword, therefore an additional a in the name\n",
        "print(Jreg(X, y, theta, lambdaa))  # should be 0.6931471805599453\n",
        "np.testing.assert_array_almost_equal_nulp(Jreg(X, y, theta, lambdaa), 0.6931471805599453)\n",
        "print(gradReg(X, y, theta, lambdaa))  # should give [[-0.1] [-12.009216589291151] [-11.262842205513593]]\n",
        "np.testing.assert_array_almost_equal_nulp(gradReg(X, y, theta, lambdaa), \n",
        "      [[0.008474576271186442],\n",
        "       [0.018788093220338975],\n",
        "       [7.777118644070071e-05],\n",
        "       [0.05034463953635592],\n",
        "       [0.011501330787338978],\n",
        "       [0.037664847359550876],\n",
        "       [0.018355987221154245],\n",
        "       [0.00732393391122216],\n",
        "       [0.008192444683890361],\n",
        "       [0.023476488865153248],\n",
        "       [0.0393486234391602],\n",
        "       [0.002239239066396939],\n",
        "       [0.0128600503371337],\n",
        "       [0.0030959372024053984],\n",
        "       [0.039302817110394433],\n",
        "       [0.019970746726922377],\n",
        "       [0.004329832324171364],\n",
        "       [0.0033864390190701966],\n",
        "       [0.0058382207780586105],\n",
        "       [0.004476290665122478],\n",
        "       [0.031007984901327713],\n",
        "       [0.031031244228507674],\n",
        "       [0.0010974023848666578],\n",
        "       [0.006315707966420357],\n",
        "       [0.00040850300602094134],\n",
        "       [0.007265043164341688],\n",
        "       [0.0013764617476890438],\n",
        "       [0.03879363634483877]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZZy2D3unzna",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# learning algorithm for logsitic regression (identical to the one in the hands-on about linear regression!)\n",
        "def gradientDescent(X, y, theta, lambdaa, alpha, num_iters):\n",
        "    J_history = []\n",
        "    for _ in range(num_iters):\n",
        "        theta -= alpha * gradReg(X, y, theta, lambdaa)\n",
        "        J_history.append(Jreg(X, y, theta, lambdaa))  # to allow displaying cost as a function of #iters\n",
        "    return theta, J_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDdAIIzCnznb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# perform learning, but use a more sophisticated optimizer than gradient descent: fmin_tn\n",
        "# define as function to allow reuse\n",
        "def learn(X, y, theta, lambdaa):\n",
        "    import scipy.optimize as opt\n",
        "\n",
        "    def Jreg_(theta, X, y, lamdaa):\n",
        "        return Jreg(X, y, theta, lamdaa)\n",
        "    def gradReg_(theta, X, y, lamdaa):\n",
        "        return gradReg(X, y, theta, lamdaa)\n",
        "\n",
        "    output = opt.fmin_tnc(func = Jreg_, x0 = theta.flatten(), fprime = gradReg_, args = (X, y.flatten(), lambdaa))\n",
        "    theta = output[0][:, np.newaxis]  # transform theta from vector to matrix\n",
        "    print(theta[0:5]) # only the first 5; should be [ 1.27271026  0.62529965  1.18111686 -2.01987398 -0.91743189]\n",
        "    return theta\n",
        "\n",
        "lambdaa = 1\n",
        "theta = learn(X, y, theta, lambdaa)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NqAUwGunznd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot decision boundary (define as function to allow reuse)\n",
        "def plot_decision_boundary(theta, lambdaa):\n",
        "    u = np.linspace(-1, 1.5, 50)\n",
        "    v = np.linspace(-1, 1.5, 50)\n",
        "    z = np.zeros((len(u), len(v)))\n",
        "\n",
        "    def mapFeatureForPlotting(X1, X2):\n",
        "        out = np.ones(1)\n",
        "        for i in range(1, degree + 1):\n",
        "            for j in range(i + 1):\n",
        "                out = np.hstack((out, np.multiply(np.power(X1, i - j), np.power(X2, j))))\n",
        "        return out\n",
        "\n",
        "    for i in range(len(u)):\n",
        "        for j in range(len(v)):\n",
        "            z[i,j] = np.dot(mapFeatureForPlotting(u[i], v[j]), theta)\n",
        "\n",
        "    mask = y == 1\n",
        "    X_ = df.iloc[:,:-1]  # original X\n",
        "    passed = plt.scatter(X_[mask][0], X_[mask][1])\n",
        "    failed = plt.scatter(X_[~mask][0], X_[~mask][1])\n",
        "    plt.contour(u,v,z,0, colors='red')\n",
        "    plt.xlabel('Microchip Test1')\n",
        "    plt.ylabel('Microchip Test2')\n",
        "    plt.legend((passed, failed), ('Passed', 'Failed'))\n",
        "    plt.title('Decision boundary for lambda=' + str(lambdaa))\n",
        "    plt.show()\n",
        "\n",
        "plot_decision_boundary(theta, lambdaa)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB-4aiTInznf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# accuracy (same source code as previous hands-on)\n",
        "def accuracy(X, y, theta):\n",
        "    pred = h(X, theta) >= 0.5\n",
        "    acc = np.mean(np.array(pred) == y)\n",
        "    return acc * 100\n",
        "\n",
        "print(\"accuracy = \" + str(accuracy(X, y, theta)))  # should be 83.05%"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mu-jgPmnzng",
        "colab_type": "text"
      },
      "source": [
        "Let's try different lambdaa's to see the effects of overfitting and underfitting. Plot the decision boundary and calculate the accuracy. Are the results according to your expectation?\n",
        "\n",
        "**Discussion**: With $\\lambda = 0$, we see that indeed the decision boundary is less smooth, which is some indication of overfitting, as the decision boundary is optimized for the training set and will generalize less well for other data. Later in this course, we'll introduce the so-called validation set, which is labeled data that the model is not trained for. When the model does very well for the training set, but not well for the validation set, this is a clear sign of overfitting.\n",
        "\n",
        "**Exercise**: adding more features, will allow the model to better fit the training set, so there's more risk of overfitting. Try this by setting degree=8 (and put the two tests in comments, as they will fail). This will generate more polynomials, so more features. This will show clearer that model is overfitting. Also note that higher accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjIGySCsnznh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lambdaas = [0, 1, 100, 150]\n",
        "for lambdaa in lambdaas:\n",
        "    theta = np.zeros((n, 1))\n",
        "    \n",
        "    # own gradient descent implementation\n",
        "    num_iters = 150000 # hyper parameter number of iterations\n",
        "    alpha = 0.002  # hyper parameter learning rate\n",
        "    #theta, J_history = gradientDescent(X, y, theta, lambdaa, alpha, num_iters)\n",
        "\n",
        "    # optimized learning algorithm\n",
        "    theta = learn(X, y, theta, lambdaa)\n",
        "\n",
        "    plot_decision_boundary(theta, lambdaa)\n",
        "    print(\"accuracy = \" + str(accuracy(X, y, theta)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}