{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate linear regression\n",
    "\n",
    "Watch [Linear regression with multiple variables: multiple features](https://www.youtube.com/watch?v=Q4GNLhRtZNc) (8 min). Check that you’ve understood and remembered the following statements:\n",
    "* multiple features\n",
    "* $i$: denotes the training sample; $j$: denotes which feature\n",
    "* Who spotted the small error on minute 7:26-7:32 in the video?\n",
    "\n",
    "Watch [Linear regression with multiple variables: gradient descent for multiple variables](https://www.youtube.com/watch?v=pkJjoro-b5c) (5 min). Check that you’ve understood and remembered the following statements:\n",
    "* by definition: $x_0(i) = 1$, for every sample $i$\n",
    "\n",
    "Watch [Linear regression with multiple variables: feature scaling](https://www.youtube.com/watch?v=r5E2X1JdHAU) (8 min). Check that you’ve understood and remembered the following statements:\n",
    "* feature scaling: faster convergence\n",
    "* all features >= -1 and <= 1\n",
    "* mean normalization: all features have a mean of about 0. So the first sample $X_1$ is normalized by performing $(x_1 – \\mu_1) / std_1$\n",
    "\n",
    "Watch [Linear regression with multiple variables: learning rate](https://www.youtube.com/watch?v=CYlR9oYhYuY) (8 min). Check that you’ve understood and remembered the following statements:\n",
    "* plot $J_\\Theta$ as a function of #iterations, while gradient descent is running, to see how convergence is going\n",
    "* automatic convergence test: definition convergence: $J_\\Theta$ decreases by less than 10^-3 in one iteration (better to look at the plot)\n",
    "* Reasons for gradient descent not working:\n",
    "  * no convergence: learning rate $\\alpha$ too big\n",
    "  * slow convergence: learning rate $\\alpha$ is too small (but can also $\\alpha$ too big)\n",
    "* Best practice: start with slow learning rate and every time increase it by factor of 3; find one value which is too small and one value which is too big\n",
    "\n",
    "Watch [Linear regression with multiple variables: features and polynomial regression](https://www.youtube.com/watch?v=Hwj_9wMXDVo) (7 min). Check that you’ve understood and remembered the following statements:\n",
    "* Model selection:\n",
    "  * create your own derived features, based on domain knowledge (e.g. one feature parcel surface instead of two features parcel width and parcel length)\n",
    "  * polynomial regression: use the square or cube of a feature\n",
    "  <img src=\"polynomialRegression.png\" />\n",
    "* Note that polynomial regression is linear regression of a non-linear function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on 2: Multivariate linear regression using Python/Numpy\n",
    "\n",
    "In this hands-on, you will implement linear regression with multiple variables to predict the prices of houses. Suppose you are selling your house and you want to know what a good market price would be. One way to do this is to first collect information on recent houses sold and make a model of housing prices. \n",
    "\n",
    "The file [ex1data2.txt](https://bitbucket.org/ercoargante/machinelearningcoursera/raw/3e715b8f5577e4f364dfd377e593d874a85d8d83/ex1/ex1data2.txt) contains a training set of housing prices in Portland, Oregon. The first column is the size of the house (in square feet), the second column is the number of bedrooms, and the third column is the price of the house. Save the file to the folder where thisJupyter notebook resides.\n",
    "\n",
    "Based on the code of the previous hands-on, perform a multivariate linear regression (no polynomials!). Note that because the input consists of two variables, you need to perform feature normalization. Appreciate how little you need to adapt the previously created code to realize multivariate linear regression!\n",
    "\n",
    "The following code is a copy of the previous hands-on containing the definitions of $h_\\Theta(x)$, $J(\\Theta)$ and the \"gradient descent\" learning algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypothesis (also called model)\n",
    "def h(X, theta):  # this is the way to define a function in python\n",
    "    return X.dot(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost function\n",
    "def J(X, y, theta):\n",
    "    m = len(y)\n",
    "    hyp = h(X, theta)  # shape of hyp is (97,1)\n",
    "    square_err = (hyp - y)**2\n",
    "    return 1/(2 * m) * np.sum(square_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient\n",
    "def grad(X, y, theta):\n",
    "    hyp = h(X, theta)\n",
    "    return 1/m * X.transpose().dot(hyp - y) # simultaneous update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning algorithm\n",
    "def gradientDescent(X, y, theta, alpha, num_iters):\n",
    "    J_history = []\n",
    "    for _ in range(num_iters):\n",
    "        theta -= alpha * grad(X, y, theta)\n",
    "        J_history.append(J(X, y, theta))  # to allow displaying cost as a function of #iters\n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: read the data set. In order to do this, look at the code of the previous hands-on. The difference is that this dataset has *two* features instead of one. Use python array slicing for this. Some examples:\n",
    "* ```[:, 0:2]``` denotes the first two columns, so column 0 and column 1\n",
    "* ```[:, 2]``` denotes the third column\n",
    "\n",
    "\n",
    "**Exercise**: the first feature is the size of the house in square feet. Create a scatter plot of \"the house prices against the size of house\".\n",
    "\n",
    "**Exercise**: the second feature is the number of bedrooms of the house. Create a scatter plot of \"the house prices against the number of bedrooms\".\n",
    "\n",
    "**Exercise**: perform feature normalization. What needs to be done is to subtract the mean of every feature value, as this will center the value around zero. Secondly, by dividing through the standard deviation, all features will be in the same range. As we've seen already, in python it's preferred to not use for-loops, but to use a vectorized implementation. This gives clean, short code, and execution is much faster. The code to perform the feature normalization is ```X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)```. Please, take a moment to understand this code:\n",
    "* every row of the matrix X is a sample, where the 1st element is the 1st feature, the 2nd element the 2nd feature, ...\n",
    "* we want to take the mean of a sample, so the mean of a row; the ```axis=0``` takes care that the mean is taken of a row and not of a column\n",
    "* in the same way the feature values are divided by the standard deviation\n",
    "\n",
    "It's also important to note that when the model is trained and ready to use for prediction, input used for prediction must be feature-normalized as well! This is easy to forget. Secondly, feature normalization must be done using mean and standard deviation of <u>the original training set</u>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code prepares the dataset by adding a column for $\\Theta_0$ and transforming $y$ from a vector to a matrix, just in the same way as the previous hands-on:\n",
    "```\n",
    "# prepare the training set\n",
    "ones = np.ones((m, 1))\n",
    "X = np.hstack((ones, X))  # add X_0 after feature normalization, because X_0 has an std of 0\n",
    "y = y[:, np.newaxis]  # don't need to do this to X, as X is already a matrix\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the cost function to be verify the correct implementation up till now:\n",
    "```\n",
    "# test the cost function\n",
    "theta = np.zeros([3, 1])\n",
    "print(\"cost function: \" + str(J(X, y, theta)))  # should give 65591548106.45744\n",
    "np.testing.assert_array_almost_equal_nulp(J(X, y, theta), 65591548106.45744)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: perform the learning in the same way as the previous hands-on, using ```num_iters=400``` and ```alpha=0.01```.\n",
    "\n",
    "**Exercise**: plot the cost function as a function of #iterations, using J_history, in the same way as the previous hands-on.\n",
    "\n",
    "**Exercise**: verify that the cost is 2105448288.6292474, when passing ```X``` and ```y```, using the trained value of $\\Theta$.\n",
    "\n",
    "**Exercise**: use the result of learning to predict the house price for a house size of 1650 and 3 bedrooms. Don't forget to perform feature normalization and to add a ```1``` at the front of ```X``` to cater for $\\Theta_0$. The result should be 289221.547371218097 dollar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please don’t look at it yet, but the Jupyter notebook with the solution is also available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The solution of hands-on 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariate linear regression. It is nice to see that the previously \n",
    "# defined functions for cost and gradient descent can be reused without modification\n",
    "\n",
    "############################### previously defined functions for cost and gradient descent ###############################\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# hypothesis (also called model)\n",
    "def h(X, theta):  # this is the way to define a function in python\n",
    "    return X.dot(theta)\n",
    "\n",
    "# cost function\n",
    "def J(X, y, theta):\n",
    "    m = len(y)\n",
    "    hyp = h(X, theta)  # shape of hyp is (97,1)\n",
    "    square_err = (hyp - y)**2\n",
    "    return 1/(2 * m) * np.sum(square_err)\n",
    "\n",
    "# gradient\n",
    "def grad(X, y, theta):\n",
    "    hyp = h(X, theta)\n",
    "    return 1/m * X.transpose().dot(hyp - y) # simultaneous update\n",
    "\n",
    "# learning algorithm\n",
    "def gradientDescent(X, y, theta, alpha, num_iters):\n",
    "    J_history = []\n",
    "    for _ in range(num_iters):\n",
    "        theta -= alpha * grad(X, y, theta)\n",
    "        J_history.append(J(X, y, theta))  # to allow displaying cost as a function of #iters\n",
    "    return theta, J_history\n",
    "\n",
    "\n",
    "############################### solution of exercises ###############################\n",
    "\n",
    "# your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
