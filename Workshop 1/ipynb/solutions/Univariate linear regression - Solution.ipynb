{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Univariate linear regression - Solution.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8ArOpepnyq6",
        "colab_type": "text"
      },
      "source": [
        "# Linear regression\n",
        "\n",
        "Linear regression is a statistical approach to model the relationship between the dependent variable and one ore more independent variables.\n",
        "\n",
        "Example (see picture). Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new\n",
        "outlet. The chain already has trucks in various cities and you have data for profits and populations from the cities. You would like to use this data to help you select which city to expand to next.\n",
        "\n",
        "<img src=\"https://github.com/sjoerdteunisse/Avans-A.I-Minor/blob/master/Workshop%201/images/linearRegression.png?raw=true\" />\n",
        "\n",
        "Linear regression is a simple algorithm that we use to illustrate many widely used machine learning (ML) concepts. The picture below shows these concepts. This allows you to get familiar with these concepts before looking at more advanced ML algorithms. This workshop is based on Andrew Ng’s Machine Learning Coursera course. While watching the videos, make notes of the parts that you don’t understand. We’ll discuss these in class. \n",
        "\n",
        "<img src=\"https://github.com/sjoerdteunisse/Avans-A.I-Minor/blob/master/Workshop%201/images/mindmap.jpg?raw=true\" />\n",
        "\n",
        "Hands-on activities in this workshop:\n",
        "* linear univariate regression using python/numpy\n",
        "* linear multivariate regression using python/numpy\n",
        "* linear univariate regression using keras\n",
        "* linear regression in practice\n",
        "\n",
        "\n",
        "##\tUnivariate linear regression\n",
        "\n",
        "Watch [Linear regression with one variable: model representation](https://www.youtube.com/watch?v=kHwlB_j7Hkc) (8 min). Check that you’ve understood the following statements:\n",
        "* Machine learning: computer learns without being explicitly programmed\n",
        "* Goals of this class: llearn ML tools, but also learn how/when to apply the ML tools (use your time efficiently)\n",
        "* Supervised learning: for each data sample, the correct answer is given; we’re teaching the computer\n",
        "* Regression: predict real-value output (as opposed to predicting a category)\n",
        "* Univariate regression: regression with one input variable\n",
        "* $x$: input variable (aka feature)\n",
        "* $y$: output variable\n",
        "* $h$: hypothesis (also called model), a function that maps input to output, $h\\colon x\\to y$\n",
        "* Initial choice of model for $h$ is a linear function: $h_\\Theta(x) = \\Theta_0 + \\Theta_1 * x$\n",
        "* $\\Theta_0$ and $\\Theta_1$ are called the **parameters** of the model; to fit the model to the data set, we’re going to determine the parameters $\\Theta_0$ and $\\Theta_1$\n",
        "* Goal of machine learning: learn $h$ using a training set\n",
        "* Don’t confuse linear function and linear regression (we'll come back to that later)\n",
        "\n",
        "Watch [Linear regression with one variable: cost function](https://www.youtube.com/watch?v=yuH4iRcggMw) (8 min). Check that you’ve understood the following statements:\n",
        "* squared error cost function:\n",
        "<img src=\"https://github.com/sjoerdteunisse/Avans-A.I-Minor/blob/master/Workshop%201/images/squaredErrorCostFunction.png?raw=true\" />\n",
        "* minimize the cost function:\n",
        "<img src=\"https://github.com/sjoerdteunisse/Avans-A.I-Minor/blob/master/Workshop%201/images/minimizeCostFunction.png?raw=true\" />\n",
        "\n",
        "Watch [Linear regression with one variable: cost function Intuition #1](https://www.youtube.com/watch?v=yR2ipCoFvNo) (11 min).\n",
        "\n",
        "Watch [Linear regression with one variable: cost function Intuition #2](https://www.youtube.com/watch?v=0kns1gXLYg4) (8 min). Check that you’ve understood the following statements:\n",
        "* Graph of cost function as a function of $\\Theta_0$ and $\\Theta_1$ \n",
        "  <img src=\"https://github.com/sjoerdteunisse/Avans-A.I-Minor/blob/master/Workshop%201/images/costFunctionGraph.png?raw=true\" alt=\"drawing\" width=\"400\"/>\n",
        "* Contour plot to visualize a cost function as function of two parameters $\\Theta_0$ and $\\Theta_1$\n",
        "* ML: have an efficient algorithm to minimize cost function $J(\\Theta_0, \\Theta_1)$\n",
        "\n",
        "\n",
        "## Parameter learning\n",
        "\n",
        "Watch [Linear regression with one variable: gradient descent](https://www.youtube.com/watch?v=F6GSRDoB-Cg) (11 min). Check that you’ve understood the following statements:\n",
        "* Gradient descent algorithm for learning\n",
        "  <img src=\"https://github.com/sjoerdteunisse/Avans-A.I-Minor/blob/master/Workshop%201/images/gradientDescentGraph.png?raw=true\" alt=\"drawing\" width=\"400\"/>\n",
        "* Using gradient descent, you can end up in a local minimum\n",
        "* Learning rate $\\alpha$\n",
        "* Simultaneous update of $\\Theta_0$ and $\\Theta_1$\n",
        "\n",
        "Watch [Linear regression with one variable: gradient descent intuition](https://www.youtube.com/watch?v=YovTqTY-PYY) (11 min). Check that you’ve understood the following statements:\n",
        "* Too small learning rate $\\alpha$: slow learning; too large learning rate $\\alpha$: overshoot minimum, possible leading to divergence\n",
        "* Gradient descent can converge to a local minimum, even if the learning rate is fixed. Because closer to the minimum the slope gets less, so gradient descent will automatically take smaller steps.\n",
        "\n",
        "Watch [Linear regression with one variable: gradient descent for linear regression](https://www.youtube.com/watch?v=GtSf2T6Co80) (10 min). Check that you’ve understood the following statements:\n",
        "* Cost function for linear regression is convex: no danger for ending up in a local minimum\n",
        "* Batch gradient descent: each step of gradient descent uses all training examples\n",
        "* ML is iterative; you can also use an analytic (numerical) approach, so without learning. Gradient descent works more efficiently for large data sets.\n",
        "\n",
        "\n",
        "## Linear algebra\n",
        "\n",
        "Watch [Linear algebra: matrices and vectors](https://www.youtube.com/watch?v=Dft1cqjwlXE) (8 min).\n",
        "\n",
        "Watch [Linear algebra: addition and scalar multiplication](https://www.youtube.com/watch?v=4WP6jVGIn7M) (6 min).\n",
        "\n",
        "Watch [Linear algebra: matrix vector multiplication](https://www.youtube.com/watch?v=gPegoVYp64w) (13 min). Check that you’ve understood the following statements:\n",
        "* Python programming: avoid for-loops and use vectorization instead, as it is shorter, clearer and faster\n",
        "\n",
        "Watch [Linear algebra: matrix-matrix multiplication](https://www.youtube.com/watch?v=_lrHXJRukMw) (11 min)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IM2N4XKRnyq7",
        "colab_type": "text"
      },
      "source": [
        "# Hands-on 1: Univariate linear regression using Python/Numpy\n",
        "\n",
        "**Installation of python:**\n",
        "* You can install python from https://www.python.org/downloads/, but it is better to use the anaconda distribution https://www.anaconda.com/distribution/, because you’ll get all the necessary libraries pre-installed. \n",
        "* Anaconda also installs Jupyter Notebook. This is a web-based interactive environment that we will use.\n",
        "* If you get an error “HTPPS ...”, install OpenSSL version 1.1.1c 64-bit and reboot.\n",
        "* Alternatively, you can use a Jetbrains PyCharm, a well-known Python IDE.\n",
        "\n",
        "Nice quickstart to the mathematical python library numpy: https://docs.scipy.org/doc/numpy/user/quickstart.html\n",
        "\n",
        "**Case description:**\n",
        "\n",
        "Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet. The chain already has trucks in various cities and you have data for profits and populations from the cities. You would like to use this data to help you select which city to expand to next.\n",
        "\n",
        "The file [ex1data1.txt](https://bitbucket.org/ercoargante/machinelearningcoursera/raw/3e715b8f5577e4f364dfd377e593d874a85d8d83/ex1/ex1data1.txt) contains the dataset for our linear regression problem in csv-format. The first column is the population of a city and the second column is the profit of a food truck in that city. A negative value for profit indicates a loss. Save the file to the folder where thisJupyter notebook resides.\n",
        "\n",
        "The following code shows how to load the dataset into a Pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLcbChgnnyq8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# read the training set\n",
        "data = pd.read_csv(\"ex1data1.txt\", header=None)  # read from dataset into Pandas DataFrame variable\n",
        "data.head()  # view first few rows of the data\n",
        "X = data.iloc[:, 0]  # read first column; upper case for matrix\n",
        "y = data.iloc[:, 1]  # read second column; lower case for vector\n",
        "m = len(y)  # number of training samples; lower case for scalar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ADjjCmenyq-",
        "colab_type": "text"
      },
      "source": [
        "Before starting on any task, it is often useful to understand the data by\n",
        "visualizing it. For this dataset, you can use a scatter plot to visualize the\n",
        "data, since it has only two properties to plot (profit and population)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSUOD07snyq-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualize the training set using the matplotlib library\n",
        "plt.scatter(X, y)\n",
        "plt.xlabel('Population of City in 10,000s')\n",
        "plt.ylabel('Profit in $10,000s')\n",
        "plt.title('Profit as a function of city population')\n",
        "plt.show()  # show the plot when preparation of the graph is ready"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dV21Or_-nyrA",
        "colab_type": "text"
      },
      "source": [
        "Python is a dynamically-typed language, whereas Java and C# are statically typed. The following commands can be handy if you lost track of the type of a variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NPbe0xPnyrA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(X.shape)  # shape of pandas dataframe or numpy ndarray (n-dimensional array)\n",
        "type(X)  # python built-in function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18OVmTRinyrC",
        "colab_type": "text"
      },
      "source": [
        "We add an additional first column to X and set it to all ones. This allows us to treat $\\Theta_0$ as simply another 'feature'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "jNnwE9FtnyrD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prepare the training set\n",
        "X = X[:, np.newaxis]  # convert from shape (97,) to (97,1), so from vector to matrix\n",
        "y = y[:, np.newaxis]  # convert from shape (97,) to (97,1), so from vector to matrix\n",
        "ones = np.ones((m, 1))  # matrix of shape (97,1) with only 1's\n",
        "X = np.hstack((ones, X))  # adding the x_0 column, so shape will be (97,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1ru7SUPnyrE",
        "colab_type": "text"
      },
      "source": [
        "Python is an interpreted programming language, in contrast to compiled programming languages like Java or C++. An interpreted programming language means that a line of source code is directly executed when it is read by the python interpreter. This means that you can rerun an individual cell of this Jupyter notebook, which is very convenient.\n",
        "\n",
        "However, realize that statements that assign a variable to itself, keep on having an effect. The only way out is to restart the kernel and rerun the complete Jupyter notebook. Examples of a statements that assign a variable to itself can be seem in the four lines of source code, just above. For example, every time you rerun ```X = np.hstack((ones, X))``` a column of ones is prepended to the front of the matrix X."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-c00Es1nyrF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(X.shape)  # shape of pandas dataframe or numpy ndarray (n-dimensional array)\n",
        "type(X)  # python built-in function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8jIY_EwnyrG",
        "colab_type": "text"
      },
      "source": [
        "**Exercise**: provide the body of the function $h_\\Theta(x)$. Remember that the hypothesis $h$ is a linear function: $h_\\Theta(x) = \\Theta_0 + \\Theta_1 * x$. We've added a column of ones to $X$, meaning that we can express the hypothesis $h$ to be: $h_\\Theta(x) = \\Theta_0 * x_0 + \\Theta_1 * x_1$. This means that $h$ is the dot product of $X$ and $\\Theta$.\n",
        "\n",
        "\n",
        "Matrix $X$ has shape (97, 2). Vector $\\Theta$ has shape (2, 1). Vector $h$ should have shape (97, 1). Use numpy dot product to calculate $h$ and return it as the function result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kKO8eO7nyrH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hypothesis (also called model)\n",
        "def h(X, theta):  # this is the way to define a function in python\n",
        "    return X.dot(theta)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHUx3O0WnyrJ",
        "colab_type": "text"
      },
      "source": [
        "**Exercise**: provide the body of the cost function $J(\\Theta)$ according to the mathematical formula in the picture just below. Note that $J(\\Theta)$ is a scalar. What you should do:\n",
        "* calculate $h_\\Theta(x)$ using the previously defined function and store it in a variable ```hyp```. Vector ```hyp``` has shape (97, 1). \n",
        "* calculate the squared error vector, also of shape (97, 1). \n",
        "* use ```np.sum``` to perform the summation, which yields a scalar. Multiply by ```1/(2 * m)```. Return this as the function result.\n",
        "\n",
        "<img src=\"https://github.com/sjoerdteunisse/Avans-A.I-Minor/blob/master/Workshop%201/images/squaredErrorCostFunction.png?raw=true\" />"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVZC49NLnyrJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cost function\n",
        "def J(X, y, theta):\n",
        "    m = len(y)\n",
        "    hyp = h(X, theta)  # shape of hyp is (97,1)\n",
        "    square_err = (hyp - y)**2  # shape of square_err is (97,1)\n",
        "    return 1/(2 * m) * np.sum(square_err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7kclQD9nyrL",
        "colab_type": "text"
      },
      "source": [
        "**Exercise**: provide the body of the gradient according to the mathematical formula in the picture just below. Note that we use $h_\\Theta(x) = \\Theta_0 * x_0 + \\Theta_1 * x_1$, where $x_0$ always has the value $1$, to allow uniform handling of the terms. Also note that the gradient is a vector with the same dimensions as $\\Theta$, so (2, 1) in this case. What you should do:\n",
        "* calculate $h_\\Theta(x)$ using the previously defined function and store it in a variable ```hyp```. Vector ```hyp``` has shape (97, 1). \n",
        "* calculate the dot product of ```X.transpose()``` and ```hyp - y```, multiply by ```1/m```, and return this as the function result.\n",
        "\n",
        "<img src=\"https://github.com/sjoerdteunisse/Avans-A.I-Minor/blob/master/Workshop%201/images/gradientDescentAlgorithm2.png?raw=true\" />"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDwJvYBxnyrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gradient\n",
        "def grad(X, y, theta):\n",
        "    hyp = h(X, theta)\n",
        "    return 1/m * X.transpose().dot(hyp - y) # simultaneous update"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQis35oGnyrN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# freehand test the cost function and gradient\n",
        "theta = np.zeros([2, 1])  # initialize theta\n",
        "print(\"cost function: \" + str(J(X, y, theta)))  # should give 32.072733877455676\n",
        "np.testing.assert_array_almost_equal_nulp(J(X, y, theta), 32.072733877455676)\n",
        "print(grad(X, y, theta))  # should give [[-5.83913505154639] [-65.32884974555672]]\n",
        "np.testing.assert_array_almost_equal_nulp(grad(X, y, theta), [[-5.83913505154639], [-65.32884974555672]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G12zhTTonyrP",
        "colab_type": "text"
      },
      "source": [
        "**Exercise**: now we're going to implement the learning algorithm to learn the parameter $\\Theta$:\n",
        "<img src=\"https://github.com/sjoerdteunisse/Avans-A.I-Minor/blob/master/Workshop%201/ipynb/solutions/gradientDescentAlgorithm.png?raw=1\" />\n",
        "* create an empty list: ```J_history = []``` (not needed for the algorithm itself, but only needed to see how $J(\\Theta)$ evolves during learning).\n",
        "* create a for-loop for the number of iterations\n",
        "* calculate the gradient\n",
        "* update $\\Theta$ using the calculated gradient and learning rate $\\alpha$\n",
        "* add the cost to the history: ```J_history.append(J(X, y, theta))```\n",
        "* not that python allows to return multiple results: ```return theta, J_history```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBr0NXLHnyrP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# learning algorithm\n",
        "def gradientDescent(X, y, theta, alpha, num_iters):\n",
        "    J_history = []\n",
        "    for _ in range(num_iters):\n",
        "        theta -= alpha * grad(X, y, theta)\n",
        "        J_history.append(J(X, y, theta))  # to allow displaying cost as a function of #iters\n",
        "    return theta, J_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKZ2tL7knyrR",
        "colab_type": "text"
      },
      "source": [
        "Before running the learning algorithm, let's first investigate the cost function by plotting it as surface plot and as contour plot. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPDZD3NinyrS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# investigating the cost function: contour plot\n",
        "theta_0_vals = np.arange(-6.0, 2.0, 0.1)\n",
        "theta_1_vals = np.arange(0.0, 2.0, 0.1)\n",
        "J_vals = [ J(X, y, [[i],[j]]) for i in theta_0_vals for j in theta_1_vals ]\n",
        "J_vals = np.array(J_vals)\n",
        "J_vals = np.reshape(J_vals, (len(theta_0_vals), len(theta_1_vals)))\n",
        "cs = plt.contour(theta_1_vals, theta_0_vals, J_vals, np.arange(4, 15, 1))\n",
        "plt.clabel(cs, inline=1, fontsize=8)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxayhtsEnyrT",
        "colab_type": "text"
      },
      "source": [
        "The contour plot shows that the cost function is concave and consequently has only one minimum. No risk of getting stuck in a local minimum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "R-3vwnWwnyrU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# perform learning\n",
        "num_iters = 1500 # hyper parameter number of iterations\n",
        "alpha = 0.01  # hyper parameter learning rate\n",
        "theta, J_history = gradientDescent(X, y, theta, alpha, num_iters)\n",
        "print(\"theta: \" + str(theta) + \", J(X, y, theta): \" + str(J(X, y, theta)))  # theta should be [[-3.63029143940436], [1.166362350335582]\n",
        "np.testing.assert_array_almost_equal_nulp(theta, [[-3.63029143940436], [1.166362350335582]])\n",
        "    \n",
        "# plot cost function as a function of #iterations\n",
        "plt.plot(J_history)\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"$J(\\Theta)$\")\n",
        "plt.title(\"Cost function convergence as a function of #iterations\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g72ZNbMNnyrV",
        "colab_type": "text"
      },
      "source": [
        "Note that the values of $\\Theta$ where $J(\\Theta)$ is minimal corresponds with the contour plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9olz7i_nyrW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualize how the learned model fits the training data\n",
        "plt.scatter(X[:,1], y)\n",
        "plt.plot(X[:,1], h(X, theta), color=\"r\")\n",
        "plt.xlabel('Population of City in 10,000s')\n",
        "plt.ylabel('Profit in $10,000s')\n",
        "plt.title('Profit as a function of city population')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSShi94BnyrX",
        "colab_type": "text"
      },
      "source": [
        "**Exercise**: now that we've learned the model, let's use it! Calculate the expected profit for a town of 35000 people (3.5 as it is in units of 10000 people). This can be done by passing ```np.array([1, 3.5])``` to $h_\\Theta(x)$, where the ```1``` is for $\\Theta_0$. This should give 4519.7678677017675 dollar. Calculate the expected profit for a town of 70000 people. This should give 45342.45012944714 dollar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oEcj_IEnyrY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###### use the result of learning to predict profit\n",
        "predict1 = h(np.array([1, 3.5]), theta) * 10000  # 35000 people\n",
        "print(\"For population = 35000, we predict a profit of \" + str(predict1) + \" dollar\")  # should give 4519.7678677017675 dollar\n",
        "np.testing.assert_array_almost_equal_nulp(predict1, 4519.7678677017675)\n",
        "    \n",
        "predict2 = h(np.array([1, 7]), theta) * 10000  # 70000 people\n",
        "print(\"For population = 70000, we predict a profit of \" + str(predict2) + \" dollar\")  # should give 45342.45012944714 dollar\n",
        "np.testing.assert_array_almost_equal_nulp(predict2, 45342.45012944714)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGnbUg1snyrZ",
        "colab_type": "text"
      },
      "source": [
        "Please don’t look at it yet, but [this](TODO) is the Jupyter notebook that contains a solution. "
      ]
    }
  ]
}